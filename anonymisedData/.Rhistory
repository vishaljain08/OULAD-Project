---
title: "Prediction of student dropout"
author: "Agnieszka Gado≈õ"
date: "1.12.2021"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
explanation of the chosen predictive analysis methods along with a R script that shows the code used to obtain the pursued analysis outcomes
## undecided machine learning model
The goal with this part was to predict final student result in a subject, based on their data, whether it would be failing, passing or withdrawing. There are three types of data connected to students in OULAD: background, VLE usage and assessments. However if we'd use assessment data, it wouldn't be predicting anymore, only calculating. Therefore we decided to use the other two to see if it's possible.
Loading libraries and data
```{r}
# loading the tidyverse and caret packages
library(tidyverse)
library(caret)
library(readr)
library(doSNOW)
library(randomForest)
# Loading the data
studentInfo <- read_csv("/OULAD/studentInfo.csv")
studentVle <- read_csv("/OULAD/studentVle.csv")
```
The first thing we have to do is select relevant variables and merge students' info with their aggregated by subject VLE activity. We are going to use _gender, region,          highest_education, imd_band, age_band, num_of_prev_attempts, studied_credits, disability_ from student info and _sum_click_ from VLE activity as predictor variables.
```{r}
#selecting variables
studentInfo <- studentInfo %>% select(code_module, code_presentation, id_student, gender, region,
highest_education, imd_band,
age_band, num_of_prev_attempts, studied_credits,
disability, final_result)
studentVle <- studentVle %>% select(code_module, code_presentation, id_student, sum_click)
#sum by group number of clicks in VLE by each student
studentVle <- aggregate(x = studentVle$sum_click, by = list(studentVle$code_module, studentVle$id_student, studentVle$code_presentation), FUN = sum)
studentVle <- studentVle %>% rename(code_module = Group.1, id_student = Group.2, code_presentation=Group.3, sum_click=x)
#join both tables to have student background with his VLE activity
df <- inner_join(studentInfo, studentVle, by = c("id_student", "code_module", "code_presentation"))
```
Now, we clean the data eliminating rows with missing data and variables with zero variability.
```{r}
# eliminating any rows that have any missing data
df <- na.omit(df)
# run the nearZeroVar function to determine
# if there are variables with NO variability
nearZeroVar(df, saveMetrics = TRUE)
# converting the text variables in dataset into factors
df <-  df %>%  mutate_if(is.character, as.factor)
```
Fortunately all variables have variability greater then zero, so we don't lose any. Next we split data to obtain training and testing sets, by randomly selecting numbers of rows which will be in training set. To ensure reproducibility of data partition we begin with setting seed.
```{r}
#seed to ensure the reproducibility of data partition
set.seed(2020)
#creating new object to split the data for training and testing sets
trainIndex <- createDataPartition(df$final_result,
p = .3,
list = FALSE,
times = 1)
#additional variable to facilitate splitting
df <-  df %>%  mutate(temp_id = 1:28174)
#rows randomly selected in trainIndex will be in training set
df_train <-  df %>%  filter(temp_id %in% trainIndex)
#remaining rows will be in testing set
df_test <-  df %>%  filter(!temp_id %in% trainIndex)
#removing additional variable
df_test <-  df_test %>%  select(-temp_id)
df_train <- df_train %>% select(-temp_id)
df <- df %>% select(-temp_id)
```
With both sets ready, we proceed to training the model. Since predicting students' results requires multi-class classification and supervised learning we selected Random Tree Model. The method XXXX, which was chosen through trial and error method while searching for best accuracy.
```{r}
#train control
train_control <-  trainControl(method = "repeatedcv", number = 10, repeats = 10)
#switching on parallel computing
cl <- makeCluster(4, type = "SOCK")
registerDoSNOW(cl)
#training the model
rf_fit <- train(final_result ~ .,
data = df_train,
method = "cforest",
metric = "Accuracy")
#stop paralel computing
stopCluster(cl)
# summary of the model
rf_fit
# Plotting predictor importance (requires library(randomForest) to be installed)
ggplot(varImp(rf_fit))
```
Without great surprises, predictor with greatest importance turned out to be _sum_clicks_, which confirms (and gives hope) that the most important factor in learning is involvement (?) and not background.
At the end we want to test the model on remaining, "fresh" data to see if it's universal or not.
```{r}
#creating a new object for the testing data including predicted values
df_test_augmented <-  df_test %>%  mutate(pred = predict(rf_fit2, df_test),
obs = final_result)
# Transform this new object into a data frame
defaultSummary(as.data.frame(df_test_augmented))
plot(df_test_augmented$pred, df_test_augmented$obs,col="lightblue")
abline(lm(df_test_augmented$obs ~ df_test_augmented$pred), col="red", lwd=3)
```
## Conlcusions
At the stage where we are right now, as of Saturday, 4th of December, the accuracy achieved on training set of data is about 58%, with a bit lower score achieved on testing set. This is not satisfactory in any way, so we'll strive to achieve better results with tuning the model. However, it's important to bear in mind, that the predictor variables picked out of availble ones in OULAD may not be enough to properly predict results, simply because the connection may be too weak.
install.packages("randomForest")
install.packages("randomForest")
# loading the tidyverse and caret packages
library(tidyverse)
library(caret)
library(readr)
library(doSNOW)
library(randomForest)
# Loading the data
studentInfo <- read_csv("/OULAD/studentInfo.csv")
studentVle <- read_csv("/OULAD/studentVle.csv")
#selecting variables
studentInfo <- studentInfo %>% select(code_module, code_presentation, id_student, gender, region,
highest_education, imd_band,
age_band, num_of_prev_attempts, studied_credits,
disability, final_result)
studentVle <- studentVle %>% select(code_module, code_presentation, id_student, sum_click)
#sum by group number of clicks in VLE by each student
studentVle <- aggregate(x = studentVle$sum_click, by = list(studentVle$code_module, studentVle$id_student, studentVle$code_presentation), FUN = sum)
studentVle <- studentVle %>% rename(code_module = Group.1, id_student = Group.2, code_presentation=Group.3, sum_click=x)
#join both tables to have student background with his VLE activity
df <- inner_join(studentInfo, studentVle, by = c("id_student", "code_module", "code_presentation"))
# eliminating any rows that have any missing data
df <- na.omit(df)
# run the nearZeroVar function to determine
# if there are variables with NO variability
nearZeroVar(df, saveMetrics = TRUE)
# converting the text variables in dataset into factors
df <-  df %>%  mutate_if(is.character, as.factor)
#seed to ensure the reproducibility of data partition
set.seed(2020)
#creating new object to split the data for training and testing sets
trainIndex <- createDataPartition(df$final_result,
p = .3,
list = FALSE,
times = 1)
#additional variable to facilitate splitting
df <-  df %>%  mutate(temp_id = 1:28174)
#rows randomly selected in trainIndex will be in training set
df_train <-  df %>%  filter(temp_id %in% trainIndex)
#remaining rows will be in testing set
df_test <-  df %>%  filter(!temp_id %in% trainIndex)
#removing additional variable
df_test <-  df_test %>%  select(-temp_id)
df_train <- df_train %>% select(-temp_id)
df <- df %>% select(-temp_id)
#train control
train_control <-  trainControl(method = "repeatedcv", number = 10, repeats = 10)
#switching on parallel computing
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
#training the model
rf_fit <- train(final_result ~ .,
data = df_train,
method = "cforest",
metric = "Accuracy")
#stop paralel computing
stopCluster(cl)
# summary of the model
rf_fit
# Plotting predictor importance (requires library(randomForest) to be installed)
ggplot(varImp(rf_fit))
#creating a new object for the testing data including predicted values
df_test_augmented <-  df_test %>%  mutate(pred = predict(rf_fit2, df_test),
obs = final_result)
# Transform this new object into a data frame
defaultSummary(as.data.frame(df_test_augmented))
plot(df_test_augmented$pred, df_test_augmented$obs,col="lightblue")
abline(lm(df_test_augmented$obs ~ df_test_augmented$pred), col="red", lwd=3)
setwd("C:/Users/visha/OneDrive - Coventry University/UPV/ADE/Project/anonymisedData")
# loading the tidyverse and caret packages
library(tidyverse)
library(caret)
library(readr)
library(doSNOW)
library(randomForest)
# Loading the data
studentInfo <- read_csv("studentInfo.csv")
studentVle <- read_csv("studentVle.csv")
#selecting variables
studentInfo <- studentInfo %>% select(code_module, code_presentation, id_student, gender, region,
highest_education, imd_band,
age_band, num_of_prev_attempts, studied_credits,
disability, final_result)
studentVle <- studentVle %>% select(code_module, code_presentation, id_student, sum_click)
#sum by group number of clicks in VLE by each student
studentVle <- aggregate(x = studentVle$sum_click, by = list(studentVle$code_module, studentVle$id_student, studentVle$code_presentation), FUN = sum)
studentVle <- studentVle %>% rename(code_module = Group.1, id_student = Group.2, code_presentation=Group.3, sum_click=x)
#join both tables to have student background with his VLE activity
df <- inner_join(studentInfo, studentVle, by = c("id_student", "code_module", "code_presentation"))
# eliminating any rows that have any missing data
df <- na.omit(df)
# run the nearZeroVar function to determine
# if there are variables with NO variability
nearZeroVar(df, saveMetrics = TRUE)
# converting the text variables in dataset into factors
df <-  df %>%  mutate_if(is.character, as.factor)
#seed to ensure the reproducibility of data partition
set.seed(2020)
#creating new object to split the data for training and testing sets
trainIndex <- createDataPartition(df$final_result,
p = .3,
list = FALSE,
times = 1)
#additional variable to facilitate splitting
df <-  df %>%  mutate(temp_id = 1:28174)
#rows randomly selected in trainIndex will be in training set
df_train <-  df %>%  filter(temp_id %in% trainIndex)
#remaining rows will be in testing set
df_test <-  df %>%  filter(!temp_id %in% trainIndex)
#removing additional variable
df_test <-  df_test %>%  select(-temp_id)
df_train <- df_train %>% select(-temp_id)
df <- df %>% select(-temp_id)
#train control
train_control <-  trainControl(method = "repeatedcv", number = 10, repeats = 10)
#switching on parallel computing
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
#training the model
rf_fit <- train(final_result ~ .,
data = df_train,
method = "cforest",
metric = "Accuracy")
